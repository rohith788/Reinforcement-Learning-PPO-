{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "WP3_CLaYDtUi",
    "outputId": "370b7c08-e286-412b-eea7-11f510df5278"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MyG7f79kTO7"
   },
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "  def __init__(self, name: str, obs, action):\n",
    "    self.obs_shape = obs\n",
    "    self.act_space = action\n",
    "\n",
    "  # def build_policy(self, name: str):\n",
    "    with tf.variable_scope(name):\n",
    "      self.obs = tf.placeholder(dtype = tf.float32, shape = [None] + list(self.obs_shape.shape), name = 'obs' )\n",
    "      with tf.variable_scope('policy_net'):\n",
    "        l_1 = tf.layers.dense(inputs = self.obs, units = 20, activation = tf.tanh)\n",
    "        l_2 = tf.layers.dense(inputs = l_1, units = 20, activation = tf.tanh)\n",
    "        l_3 = tf.layers.dense(inputs = l_2, units = 20, activation = tf.tanh)\n",
    "        self.p_of_A = tf.layers.dense(inputs = tf.divide(l_3, 0.1), units = self.act_space.n, activation = tf.nn.softmax)\n",
    "\n",
    "      with tf.variable_scope('val_net'):\n",
    "        f_l = tf.layers.dense(inputs = self.obs, units = 20, activation = tf.tanh)\n",
    "        s_l = tf.layers.dense(inputs = f_l, units = 20, activation = tf.tanh)\n",
    "        self.val_prediction = tf.layers.dense(inputs = s_l, units = 1, activation = None)\n",
    "\n",
    "      self.stochastic = tf.multinomial(tf.log(self.p_of_A), num_samples = 1)\n",
    "      self.stochastic = tf.reshape(self.stochastic, shape = [-1])\n",
    "      self.deterministic = tf.argmax(self.p_of_A, axis = 1)\n",
    "      self.scope = tf.get_variable_scope().name\n",
    "\n",
    "  def act(self, obs, stochastic = True):\n",
    "    if stochastic:\n",
    "      return tf.get_default_session().run([self.stochastic, self.val_prediction], feed_dict = {self.obs : obs})\n",
    "    return tf.get_default_session().run([self.deterministic, self.val_prediction], feed_dict = {self.obs : obs})\n",
    "\n",
    "  def get_trainable_variables(self):\n",
    "    return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ppo_agent:\n",
    "  def __init__(self, env, p, op):\n",
    "    self.Policy_net = p\n",
    "    self.Old_policy_net = op\n",
    "    self.gamma = 0.95\n",
    "\n",
    "    p = self.Policy_net.get_trainable_variables()\n",
    "\n",
    "    op = self.Old_policy_net.get_trainable_variables()\n",
    "\n",
    "    with tf.variable_scope('assign_old'): #assigning the parameters for the Old Policy Network\n",
    "      self.assign_old = []\n",
    "      for old_val, val in zip(op, p):\n",
    "        self.assign_old.append(tf.assign(old_val, val))\n",
    "    \n",
    "    with tf.variable_scope('train_ip'):\n",
    "      self.A = tf.placeholder(dtype = tf.int32, shape = [None], name = 'actions')\n",
    "      self.R = tf.placeholder(dtype = tf.float32, shape = [None], name = 'rewards')\n",
    "      self.Val_N = tf.placeholder(dtype = tf.float32, shape = [None], name = 'next_val_prediction')\n",
    "      self.gaes = tf.placeholder(dtype = tf.float32, shape = [None], name = 'gaes') #generative advantage estimator\n",
    "\n",
    "    A_predict = self.Policy_net.p_of_A #output layer of the nn\n",
    "    old_A_predict = self.Old_policy_net.p_of_A #output layer of the nn\n",
    "\n",
    "    #Prediction from the policy network\n",
    "    A_predict = A_predict * tf.one_hot(indices = self.A, depth = A_predict.shape[1])\n",
    "    A_predict = tf.reduce_sum(A_predict, axis = 1)\n",
    "\n",
    "    #Prediction for the old network\n",
    "    old_A_predict = old_A_predict * tf.one_hot(indices = self.A, depth = old_A_predict.shape[1])\n",
    "    old_A_predict = tf.reduce_sum(old_A_predict, axis = 1)\n",
    "\n",
    "    with tf.variable_scope('loss/clip'):\n",
    "      ratios = tf.exp(tf.log(A_predict) - tf.log(old_A_predict))\n",
    "      clipped_ratios = tf.clip_by_value(ratios, clip_value_min = 1 - 0.2, clip_value_max = 1 + 0.2)#0.2 is the cliping value(epsilon in the clipping formula)\n",
    "      loss_clip = tf.minimum(tf.multiply(self.gaes, ratios), tf.multiply(self.gaes, clipped_ratios))\n",
    "      loss_clip = tf.reduce_mean(loss_clip)\n",
    "      tf.summary.scalar('loss/clip', loss_clip)\n",
    "\n",
    "    with tf.variable_scope('loss/valf'):\n",
    "      v_preds = self.Policy_net.val_prediction\n",
    "      loss_val_f = tf.squared_difference(self.R + self.gamma * self.Val_N, v_preds)\n",
    "      loss_val_f = tf.reduce_mean(loss_val_f)\n",
    "      tf.summary.scalar('loss/valf', loss_val_f)\n",
    "\n",
    "    with tf.variable_scope('loss/entropy'):\n",
    "      enthropy = -tf.reduce_sum(self.Policy_net.p_of_A*tf.log(tf.clip_by_value(self.Policy_net.p_of_A, 1e-10, 1.0)), axis = 1)\n",
    "      enthropy = tf.reduce_mean(enthropy, axis = 0)\n",
    "      tf.summary.scalar('loss/enthropy', enthropy)\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "      loss = loss_clip - (1.0) * loss_val_f + (0.01) * enthropy\n",
    "      loss = -loss\n",
    "      tf.summary.scalar('loss', loss)\n",
    "     \n",
    "\n",
    "    self.merge_summary = tf.summary.merge_all()\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.000057)\n",
    "\n",
    "\n",
    "    self.train_op = optimizer.minimize(loss, var_list = p) #computing and updating the gradients\n",
    "\n",
    "  def assign_policy_params(self):\n",
    "    return tf.get_default_session().run(self.assign_old)\n",
    "\n",
    "  def get_gaes(self, R, p_val, p_val_next):\n",
    "    # print(type(R))\n",
    "    # print(type(p_val))\n",
    "    # print(type(self.gamma))\n",
    "    delta = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(R, p_val, p_val_next)] #δt = rt + γV (st+1) − V (st) \n",
    "    # delta = [self.gamma * v_next  for r_t, v_next, v in zip(R, p_val, p_val_next)] #δt = rt + γV (st+1) − V (st) \n",
    "    gaes = copy.deepcopy(delta)\n",
    "    for t in reversed(range(len(gaes) - 1)):\n",
    "      gaes[t] = gaes[t] + self.gamma * gaes[t + 1]\n",
    "    return gaes\n",
    "\n",
    "\n",
    "\n",
    "  def summary(self,S, A, R, Val_N, gaes ):\n",
    "    return tf.get_default_session().run([self.merge_summary], feed_dict = {self.Policy_net.obs: S, self.Old_policy_net.obs : S, self.A: A, self.R: R, self.Val_N: Val_N, self.gaes: gaes})\n",
    "\n",
    "  def train(self,S, A, R, Val_N, gaes ):\n",
    "    return tf.get_default_session().run([self.train_op], feed_dict = {self.Policy_net.obs: S, self.Old_policy_net.obs : S, self.A: A, self.R: R, self.Val_N: Val_N, self.gaes: gaes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueVaZDLAUTYE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-420680265528>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-2-420680265528>:20: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "C:\\Users\\srohi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-161.0\n",
      "-179.0\n",
      "-179.33333333333334\n",
      "-178.0\n",
      "-173.8\n",
      "-171.83333333333334\n",
      "-172.0\n",
      "-170.0\n",
      "-165.88888888888889\n",
      "-164.5\n",
      "-167.1818181818182\n",
      "-169.58333333333334\n",
      "-169.15384615384616\n",
      "-170.35714285714286\n",
      "-168.4\n",
      "-167.5\n",
      "-165.11764705882354\n",
      "-164.05555555555554\n",
      "-164.52631578947367\n",
      "-164.2\n",
      "-163.28571428571428\n",
      "-163.27272727272728\n",
      "-162.04347826086956\n",
      "-162.54166666666666\n",
      "-162.8\n",
      "-162.84615384615384\n",
      "-163.37037037037038\n",
      "-164.60714285714286\n",
      "-165.24137931034483\n",
      "-165.2\n",
      "-164.2258064516129\n",
      "-164.0625\n",
      "-163.5151515151515\n",
      "-164.02941176470588\n",
      "-165.22857142857143\n",
      "-166.38888888888889\n",
      "-167.32432432432432\n",
      "-167.60526315789474\n",
      "-169.76923076923077\n",
      "-170.275\n",
      "-169.46341463414635\n",
      "-170.4047619047619\n",
      "-172.25581395348837\n",
      "-172.0\n",
      "-171.33333333333334\n",
      "-170.69565217391303\n",
      "-169.85106382978722\n",
      "-168.75\n",
      "-168.3469387755102\n",
      "-168.72\n",
      "-168.37254901960785\n",
      "-167.80769230769232\n",
      "-167.60377358490567\n",
      "-167.07407407407408\n",
      "-166.92727272727274\n",
      "-167.53571428571428\n",
      "-167.80701754385964\n",
      "-167.51724137931035\n",
      "-166.88135593220338\n",
      "-166.8\n",
      "-166.68852459016392\n",
      "-165.96774193548387\n",
      "-165.73015873015873\n",
      "-165.953125\n",
      "-165.06153846153848\n",
      "-164.87878787878788\n",
      "-164.32835820895522\n",
      "-164.23529411764707\n",
      "-164.1014492753623\n",
      "-163.82857142857142\n",
      "-164.4788732394366\n",
      "-164.69444444444446\n",
      "-165.41095890410958\n",
      "-165.05405405405406\n",
      "-164.72\n",
      "-164.6315789473684\n",
      "-164.46753246753246\n",
      "-164.4102564102564\n",
      "-163.9746835443038\n",
      "-163.525\n",
      "-163.54320987654322\n",
      "-163.2439024390244\n",
      "-163.16867469879517\n",
      "-162.83333333333334\n",
      "-162.75294117647059\n",
      "-162.15116279069767\n",
      "-162.0344827586207\n",
      "-161.6590909090909\n",
      "-161.48314606741573\n",
      "-161.27777777777777\n",
      "-160.76923076923077\n",
      "-160.3695652173913\n",
      "-160.01075268817203\n",
      "-159.5744680851064\n",
      "-159.26315789473685\n",
      "-159.1875\n",
      "-158.56701030927834\n",
      "-158.14285714285714\n",
      "-157.91919191919192\n",
      "-157.54\n",
      "-156.99\n",
      "-156.86\n",
      "-156.13\n",
      "-155.51\n",
      "-155.28\n",
      "-154.85\n",
      "-154.32\n",
      "-153.95\n",
      "-153.63\n",
      "-153.35\n",
      "-152.84\n",
      "-152.22\n",
      "-151.54\n",
      "-150.81\n",
      "-150.39\n",
      "-149.85\n",
      "-149.54\n",
      "-149.1\n",
      "-148.6\n",
      "-148.28\n",
      "-148.0\n",
      "-147.32\n",
      "-146.93\n",
      "-146.16\n",
      "-145.4\n",
      "-144.96\n",
      "-144.46\n",
      "-143.53\n",
      "-143.46\n",
      "-143.16\n",
      "-143.22\n",
      "-142.74\n",
      "-142.63\n",
      "-142.7\n",
      "-142.05\n",
      "-141.02\n",
      "-140.27\n",
      "-139.46\n",
      "-138.1\n",
      "-137.17\n",
      "-137.22\n",
      "-136.46\n",
      "-135.02\n",
      "-134.58\n",
      "-134.74\n",
      "-134.73\n",
      "-134.39\n",
      "-134.28\n",
      "-134.04\n",
      "-133.2\n",
      "-133.07\n",
      "-132.92\n",
      "-132.63\n",
      "-132.82\n",
      "-132.46\n",
      "-132.19\n",
      "-131.56\n",
      "-131.16\n",
      "-131.26\n",
      "-130.71\n",
      "-130.25\n",
      "-130.54\n",
      "-130.3\n",
      "-129.87\n",
      "-130.53\n",
      "-130.64\n",
      "-130.63\n",
      "-130.39\n",
      "-130.16\n",
      "-129.99\n",
      "-129.1\n",
      "-128.53\n",
      "-127.52\n",
      "-127.24\n",
      "-127.09\n",
      "-126.74\n",
      "-127.25\n",
      "-126.99\n",
      "-127.02\n",
      "-126.87\n",
      "-126.58\n",
      "-126.9\n",
      "-126.72\n",
      "-126.45\n",
      "-126.04\n",
      "-126.08\n",
      "-127.08\n",
      "-127.39\n",
      "-126.93\n",
      "-126.59\n",
      "-127.0\n",
      "-126.93\n",
      "-126.89\n",
      "-126.82\n",
      "-126.45\n",
      "-126.06\n",
      "-126.19\n",
      "-126.16\n",
      "-126.03\n",
      "-126.29\n",
      "-126.63\n",
      "-126.14\n",
      "-126.28\n",
      "-126.56\n",
      "-126.46\n",
      "-126.62\n",
      "-126.49\n",
      "-126.76\n",
      "-127.08\n",
      "-127.01\n",
      "-127.86\n",
      "-128.3\n",
      "-128.69\n",
      "-128.74\n",
      "-128.91\n",
      "-129.08\n",
      "-129.29\n",
      "-129.61\n",
      "-130.44\n",
      "-130.26\n",
      "-130.19\n",
      "-130.58\n",
      "-130.92\n",
      "-131.37\n",
      "-131.63\n",
      "-131.53\n",
      "-131.55\n",
      "-132.14\n",
      "-131.48\n",
      "-131.19\n",
      "-130.88\n",
      "-131.05\n",
      "-130.84\n",
      "-130.29\n",
      "-130.04\n",
      "-130.22\n",
      "-130.09\n",
      "-130.1\n",
      "-129.87\n",
      "-130.26\n",
      "-130.1\n",
      "-129.92\n",
      "-129.89\n",
      "-130.24\n",
      "-130.0\n",
      "-129.94\n",
      "-130.2\n",
      "-130.33\n",
      "-130.05\n",
      "-130.16\n",
      "-130.02\n",
      "-129.83\n",
      "-129.78\n",
      "-129.32\n",
      "-129.29\n",
      "-128.98\n",
      "-128.88\n",
      "-129.04\n",
      "-128.71\n",
      "-128.72\n",
      "-128.87\n",
      "-129.0\n",
      "-128.82\n",
      "-128.3\n",
      "-127.78\n",
      "-127.34\n",
      "-127.17\n",
      "-126.86\n",
      "-126.76\n",
      "-126.63\n",
      "-126.9\n",
      "-126.93\n",
      "-127.13\n",
      "-127.08\n",
      "-126.89\n",
      "-126.87\n",
      "-126.02\n",
      "-125.83\n",
      "-125.53\n",
      "-125.37\n",
      "-125.18\n",
      "-124.58\n",
      "-124.4\n",
      "-124.39\n",
      "-124.15\n",
      "-124.29\n",
      "-123.28\n",
      "-122.89\n",
      "-123.05\n",
      "-123.04\n",
      "-122.54\n",
      "-122.36\n",
      "-122.25\n",
      "-122.21\n",
      "-122.3\n",
      "-122.67\n",
      "-122.54\n",
      "-122.76\n",
      "-123.05\n",
      "-122.52\n",
      "-122.36\n",
      "-122.04\n",
      "-121.9\n",
      "-121.7\n",
      "-121.55\n",
      "-121.42\n",
      "-121.67\n",
      "-121.4\n",
      "-121.18\n",
      "-121.19\n",
      "-119.83\n",
      "-119.13\n",
      "-118.87\n",
      "-118.8\n",
      "-119.02\n",
      "-119.02\n",
      "-118.92\n",
      "-119.18\n",
      "-118.06\n",
      "-117.85\n",
      "-117.79\n",
      "-117.48\n",
      "-117.88\n",
      "-117.33\n",
      "-117.48\n",
      "-117.48\n",
      "-117.21\n",
      "-116.46\n",
      "-116.4\n",
      "-116.58\n",
      "-116.54\n",
      "-116.44\n",
      "-116.27\n",
      "-116.01\n",
      "-116.99\n",
      "-118.06\n",
      "-118.05\n",
      "-119.14\n",
      "-119.27\n",
      "-119.04\n",
      "-118.7\n",
      "-118.53\n",
      "-118.39\n",
      "-118.22\n",
      "-119.22\n",
      "-119.02\n",
      "-119.7\n",
      "-119.57\n",
      "-120.36\n",
      "-120.49\n",
      "-120.26\n",
      "-120.23\n",
      "-120.01\n",
      "-119.81\n",
      "-119.53\n",
      "-119.06\n",
      "-119.12\n",
      "-118.88\n",
      "-118.92\n",
      "-118.69\n",
      "-118.5\n",
      "-118.46\n",
      "-118.35\n",
      "-118.42\n",
      "-118.51\n",
      "-118.22\n",
      "-118.27\n",
      "-118.05\n",
      "-117.71\n",
      "-117.48\n",
      "-117.21\n",
      "-116.9\n",
      "-116.62\n",
      "-116.67\n",
      "-116.84\n",
      "-116.65\n",
      "-116.82\n",
      "-117.4\n",
      "-117.17\n",
      "-117.6\n",
      "-117.4\n",
      "-117.23\n",
      "-117.01\n",
      "-116.94\n",
      "-116.92\n",
      "-116.62\n",
      "-116.23\n",
      "-115.92\n",
      "-115.85\n",
      "-115.68\n",
      "-115.47\n",
      "-115.71\n",
      "-115.49\n",
      "-115.28\n",
      "-115.21\n",
      "-114.83\n",
      "-115.03\n",
      "-114.69\n",
      "-114.23\n",
      "-114.28\n",
      "-114.0\n",
      "-114.2\n",
      "-114.69\n",
      "-114.48\n",
      "-114.42\n",
      "-114.1\n",
      "-113.69\n",
      "-113.6\n",
      "-113.37\n",
      "-113.41\n",
      "-113.31\n",
      "-113.59\n",
      "-114.51\n",
      "-114.42\n",
      "-113.92\n",
      "-113.61\n",
      "-114.46\n",
      "-113.67\n",
      "-113.56\n",
      "-113.41\n",
      "-113.51\n",
      "-113.66\n",
      "-112.86\n",
      "-113.02\n",
      "-112.57\n",
      "-112.38\n",
      "-112.51\n",
      "-112.58\n",
      "-112.38\n",
      "-111.96\n",
      "-111.87\n",
      "-111.73\n",
      "-111.89\n",
      "-111.89\n",
      "-110.82\n",
      "-109.65\n",
      "-109.45\n",
      "-108.24\n",
      "-108.16\n",
      "-108.03\n",
      "-108.02\n",
      "-107.96\n",
      "-108.14\n",
      "-107.79\n",
      "-106.55\n",
      "-106.28\n",
      "-105.51\n",
      "-105.71\n",
      "-104.8\n",
      "-104.35\n",
      "-104.25\n",
      "-104.22\n",
      "-104.3\n",
      "-104.44\n",
      "-104.51\n",
      "-104.37\n",
      "-104.59\n",
      "-104.54\n",
      "-104.8\n",
      "-104.71\n",
      "-104.42\n",
      "-103.91\n",
      "-103.91\n",
      "-103.9\n",
      "-103.6\n",
      "-103.75\n",
      "-103.49\n",
      "-103.71\n",
      "-103.91\n",
      "-104.03\n",
      "-103.84\n",
      "-103.8\n",
      "-103.77\n",
      "-103.72\n",
      "-103.43\n",
      "-103.33\n",
      "-103.05\n",
      "-102.16\n",
      "-102.22\n",
      "-101.83\n",
      "-101.84\n",
      "-101.7\n",
      "-101.49\n",
      "-101.35\n",
      "-101.64\n",
      "-101.7\n",
      "-101.42\n",
      "-101.6\n",
      "-101.51\n",
      "-101.57\n",
      "-101.62\n",
      "-101.24\n",
      "-101.18\n",
      "-101.21\n",
      "-101.4\n",
      "-101.36\n",
      "-101.08\n",
      "-100.97\n",
      "-101.09\n",
      "-100.98\n",
      "-100.96\n",
      "-100.8\n",
      "-100.04\n",
      "-99.95\n",
      "Saved!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 10000\n",
    "tf.reset_default_graph()\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "obs_space = env.observation_space\n",
    "Policy_net = neural_net('new', obs_space, env.action_space)\n",
    "Old_policy_net = neural_net('old', obs_space, env.action_space)\n",
    "agent = ppo_agent(env, Policy_net, Old_policy_net)\n",
    "save = tf.train.Saver()\n",
    "stoc = True\n",
    "mov_avg = collections.deque(maxlen = 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  writer = tf.summary.FileWriter('./logacro/21', sess.graph)\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  S = env.reset()\n",
    "  R = 0\n",
    "  num = 0\n",
    "\n",
    "  for e in range(epochs):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    v_preds = []\n",
    "    steps = 0\n",
    "    while True:\n",
    "#       if e % 200 == 0:\n",
    "#         stoc = False\n",
    "#       else:\n",
    "#         stoc = True\n",
    "      steps += 1\n",
    "      S = np.stack([S]).astype(dtype = np.float32)\n",
    "      A, v_pred = Policy_net.act(obs = S, stochastic = True)\n",
    "      A = np.asscalar(A)\n",
    "      v_pred = np.asscalar(v_pred)\n",
    "\n",
    "      observations.append(S)\n",
    "      rewards.append(R)\n",
    "      actions.append(A)\n",
    "      v_preds.append(v_pred)\n",
    "\n",
    "      N_S, R, done, info = env.step(A)\n",
    "\n",
    "      if done:\n",
    "        v_preds_next = v_preds[1:] + [0]\n",
    "        S = env.reset()\n",
    "        reward = -1\n",
    "        break\n",
    "      S = N_S\n",
    "    \n",
    "    mov_avg.append(sum(rewards))\n",
    "    writer.add_summary(tf.Summary(value = [tf.Summary.Value(tag = 'episode_length', simple_value = steps)]),e)\n",
    "    writer.add_summary(tf.Summary(value = [tf.Summary.Value(tag = 'moving avg', simple_value = np.mean(mov_avg))]),e)\n",
    "    print(np.mean(mov_avg))\n",
    "    \n",
    "    if np.mean(mov_avg) > -100 and e > 99:\n",
    "        save.save(sess, './model/acro21.ckpt')\n",
    "        print(\"Saved!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        break\n",
    "    else: \n",
    "      num = 0\n",
    "    gaes = agent.get_gaes(rewards, v_preds, v_preds_next) \n",
    "\n",
    "    #converting all the lists into array for using it in tensorflow\n",
    "    observations = np.reshape(observations, newshape = [-1] + list(obs_space.shape))\n",
    "    actions = np.array(actions).astype(dtype = np.int32)\n",
    "    rewards = np.array(rewards).astype(dtype = np.float32)\n",
    "    v_preds_next = np.array(v_preds_next).astype(dtype = np.float32)\n",
    "    gaes = np.array(gaes).astype(dtype = np.float32)\n",
    "    gaes = (gaes - gaes.mean()) / gaes.std()\n",
    "\n",
    "    agent.assign_policy_params()\n",
    "\n",
    "    ip = [observations, actions, rewards, v_preds_next, gaes]\n",
    "\n",
    "    for epochs in range(4):\n",
    "      sample = np.random.randint(low = 0, high = observations.shape[0], size = 64)\n",
    "      sampled_stuff = [np.take( a = i, indices = sample, axis = 0) for i in ip]\n",
    "      agent.train(sampled_stuff[0], sampled_stuff[1], sampled_stuff[2], sampled_stuff[3], sampled_stuff[4])\n",
    "      summary = agent.summary(ip[0], ip[1], ip[2], ip[3], ip[4] )[0]\n",
    "      writer.add_summary(summary, e)\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
