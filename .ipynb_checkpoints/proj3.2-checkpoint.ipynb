{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "WP3_CLaYDtUi",
    "outputId": "370b7c08-e286-412b-eea7-11f510df5278"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MyG7f79kTO7"
   },
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "  def __init__(self, name: str, obs, action):\n",
    "    self.obs_shape = obs\n",
    "    self.act_space = action\n",
    "\n",
    "  # def build_policy(self, name: str):\n",
    "    with tf.variable_scope(name):\n",
    "      self.obs = tf.placeholder(dtype = tf.float32, shape = [None] + list(self.obs_shape.shape), name = 'obs' )\n",
    "      with tf.variable_scope('policy_net'):\n",
    "        l_1 = tf.layers.dense(inputs = self.obs, units = 20, activation = tf.tanh)\n",
    "        l_2 = tf.layers.dense(inputs = l_1, units = 20, activation = tf.tanh)\n",
    "        l_3 = tf.layers.dense(inputs = l_2, units = 20, activation = tf.tanh)\n",
    "        self.p_of_A = tf.layers.dense(inputs = tf.divide(l_3, 0.1), units = self.act_space.n, activation = tf.nn.softmax)\n",
    "\n",
    "      with tf.variable_scope('val_net'):\n",
    "        f_l = tf.layers.dense(inputs = self.obs, units = 20, activation = tf.tanh)\n",
    "        s_l = tf.layers.dense(inputs = f_l, units = 20, activation = tf.tanh)\n",
    "        self.val_prediction = tf.layers.dense(inputs = s_l, units = 1, activation = None)\n",
    "\n",
    "      self.stochastic = tf.multinomial(tf.log(self.p_of_A), num_samples = 1)\n",
    "      self.stochastic = tf.reshape(self.stochastic, shape = [-1])\n",
    "      self.deterministic = tf.argmax(self.p_of_A, axis = 1)\n",
    "      self.scope = tf.get_variable_scope().name\n",
    "\n",
    "  def act(self, obs, stochastic = True):\n",
    "    if stochastic:\n",
    "      return tf.get_default_session().run([self.stochastic, self.val_prediction], feed_dict = {self.obs : obs})\n",
    "    return tf.get_default_session().run([self.deterministic, self.val_prediction], feed_dict = {self.obs : obs})\n",
    "\n",
    "  def get_trainable_variables(self):\n",
    "    return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueVaZDLAUTYE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 10000\n",
    "tf.reset_default_graph()\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "obs_space = env.observation_space\n",
    "Policy_net = neural_net('new', obs_space, env.action_space)\n",
    "Old_policy_net = neural_net('old', obs_space, env.action_space)\n",
    "agent = ppo_agent(env, Policy_net, Old_policy_net)\n",
    "\n",
    "####saver####\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  writer = tf.summary.FileWriter('C:\\Users\\srohi\\OneDrive\\Desktop\\rl', sess.graph)\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  S = env.reset()\n",
    "  R = 0\n",
    "  num = 0\n",
    "\n",
    "  for e in range(epochs):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    v_preds = []\n",
    "    steps = 0\n",
    "    while True:\n",
    "      steps += 1\n",
    "      S = np.stack([S]).astype(dtype = np.float32)\n",
    "      A, v_pred = Policy_net.act(obs = S)\n",
    "\n",
    "      A = np.asscalar(A)\n",
    "      v_pred = np.asscalar(v_pred)\n",
    "\n",
    "      observations.append(S)\n",
    "      rewards.append(R)\n",
    "      actions.append(A)\n",
    "      v_preds.append(v_pred)\n",
    "\n",
    "      N_S, R, done, info = env.step(A)\n",
    "\n",
    "      if done:\n",
    "        v_preds_next = v_preds[1:] + [0]\n",
    "        S = env.reset()\n",
    "        reward = -1\n",
    "        break\n",
    "      S = N_S\n",
    "\n",
    "    writer.add_summary(tf.Summary(value = [tf.Summary.Value(tag = 'episode_length', simple_value = steps)]),e)\n",
    "    writer.add_summary(tf.Summary(value = [tf.Summary.Value(tag = 'total reward', simple_value = sum(rewards))]),e)\n",
    "    print(sum(rewards))\n",
    "    if sum(rewards) >= 195:\n",
    "      num += 1\n",
    "      if num >= 100:\n",
    "        saver.save(sess, './model/model.ckpt')\n",
    "        print(\"Saved!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        break\n",
    "    else: \n",
    "      num = 0\n",
    "    gaes = agent.get_gaes(rewards, v_preds, v_preds_next) \n",
    "\n",
    "    #converting all the lists into array for using it in tensorflow\n",
    "    observations = np.reshape(observations, newshape = [-1] + list(obs_space.shape))\n",
    "    actions = np.array(actions).astype(dtype = np.float32)\n",
    "    rewards = np.array(rewards).astype(dtype = np.float32)\n",
    "    v_preds_next = np.array(v_preds_next).astype(dtype = np.float32)\n",
    "    gaes = np.array(gaes).astype(dtype = np.float32)\n",
    "    gaes = (gaes - gaes.mean()) / gaes.std()\n",
    "\n",
    "    agent.assign_policy_params()\n",
    "\n",
    "    ip = [observations, actions, rewards, v_preds_next, gaes]\n",
    "\n",
    "    for epochs in range(4):\n",
    "      sample = np.random.randint(low = 0, high = observations.shape[0], size = 64)\n",
    "      sampled_stuff = [np.take( a = i, indices = sample, axis = 0) for i in ip]\n",
    "      agent.train(sampled_stuff[0], sampled_stuff[1], sampled_stuff[2], sampled_stuff[3], sampled_stuff[4])\n",
    "      summary = agent.summary(ip[0], ip[1], ip[2], ip[3], ip[4] )[0]\n",
    "      writer.add_summary(summary, e)\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "QB8Ddxf9pj2_",
    "outputId": "304d08b5-3930-41ef-9872-5f4c380e440b"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-531df276f1d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobs_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m####saver####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-f9acd8d7e782>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gaes'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#generative advantage estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mA_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_of_A\u001b[0m \u001b[0;31m#output layer of the nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mold_A_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOld_policy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_of_A\u001b[0m \u001b[0;31m#output layer of the nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'p_of_A'"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "obs_space = env.observation_space\n",
    "agent = agent(env)\n",
    "####saver####\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
